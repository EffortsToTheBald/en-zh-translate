# backend/translator.py
import os
import sys
import torch
import torch.nn.functional as F
import sentencepiece as spm

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
SRC_PATH = os.path.join(PROJECT_ROOT, 'src')
if SRC_PATH not in sys.path:
    sys.path.insert(0, SRC_PATH)

from src.config import Config
from src.model import build_model


def generate_square_subsequent_mask(sz: int) -> torch.Tensor:
    """ç”Ÿæˆ float ç±»å‹çš„ä¸‹ä¸‰è§’ maskï¼Œ-inf è¡¨ç¤ºå±è”½"""
    mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)
    return mask


class EN2ZHTranslator:
    def __init__(self, model_path=None):
        if model_path is None:
            model_path = os.path.join(Config.CHECKPOINT_DIR, "best_model.pth")
        
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        self.device = torch.device(Config.DEVICE)

        # --- 1. åŠ è½½ SentencePiece Tokenizers ---
        self.en_sp = spm.SentencePieceProcessor()
        self.zh_sp = spm.SentencePieceProcessor()
        en_model_path = os.path.join(Config.VOCAB_DIR, "en.model")
        zh_model_path = os.path.join(Config.VOCAB_DIR, "ch.model")
        self.en_sp.load(en_model_path)
        self.zh_sp.load(zh_model_path)

        # --- 2. âš ï¸ å¼ºåˆ¶ä½¿ç”¨ä¸è®­ç»ƒæ—¶ dataset.py ä¸€è‡´çš„ç‰¹æ®Š token ID ---
        # åœ¨ dataset.py ä¸­ä½ å†™çš„æ˜¯: [1] + ... + [0]ï¼Œä¸” padding_value=3
        self.sos_id = 1   # BOS / SOS
        self.eos_id = 0   # EOS ï¼ˆæ³¨æ„ï¼šè¿™é€šå¸¸æ˜¯ <unk>ï¼Œä½†ä½ çš„æ¨¡å‹æŠŠå®ƒå½“ EOSï¼‰
        self.pad_id = 3   # PAD

        # å¯é€‰ï¼šæ‰“å°ç¡®è®¤
        print(f"ğŸ”§ ä½¿ç”¨å›ºå®šç‰¹æ®Š token ID: SOS={self.sos_id}, EOS={self.eos_id}, PAD={self.pad_id}")
        print(f"ï¼ˆæ³¨æ„ï¼šè¿™è¦†ç›–äº† SentencePiece çš„é»˜è®¤å€¼ï¼‰")

        # --- 3. åŠ è½½æ¨¡å‹ checkpoint ---
        checkpoint = torch.load(model_path, map_location=self.device)
        saved_config = checkpoint["config"]

        model_kwargs = {
            'd_model': saved_config.get('D_MODEL', 512),
            'nhead': saved_config.get('NHEAD', 8),
            'num_encoder_layers': saved_config.get('NUM_ENCODER_LAYERS', 6),
            'num_decoder_layers': saved_config.get('NUM_DECODER_LAYERS', 6),
            'dim_feedforward': saved_config.get('DIM_FEEDFORWARD', 2048),
            'dropout': saved_config.get('DROPOUT', 0.1),
        }

        self.model = build_model(
            src_vocab_size=self.en_sp.vocab_size(),
            tgt_vocab_size=self.zh_sp.vocab_size(),
            device=self.device,
            **model_kwargs
        )

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval().to(self.device)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    def translate(self, sentence: str, temperature=0.8, max_len=50) -> str:
        self.model.eval()
        with torch.no_grad():
            src_pieces = self.en_sp.encode(sentence, out_type=int)
            src = torch.tensor(src_pieces).unsqueeze(0).to(self.device)

            tgt_indices = [self.sos_id]  # = [1]

            for _ in range(max_len - 1):
                tgt = torch.tensor(tgt_indices).unsqueeze(0).to(self.device)

                src_padding_mask = (src == self.pad_id).to(self.device)      # pad_id=3
                tgt_padding_mask = (tgt == self.pad_id).to(self.device)
                tgt_mask = generate_square_subsequent_mask(tgt.size(1)).to(self.device)

                output, _ = self.model(
                    src=src,
                    tgt=tgt,
                    tgt_mask=tgt_mask,
                    src_padding_mask=src_padding_mask,
                    tgt_padding_mask=tgt_padding_mask,
                    memory_key_padding_mask=src_padding_mask
                )

                next_token_logits = output[0, -1, :] / temperature

                # é˜²å¾¡ï¼šå¤„ç† NaN/Inf
                if torch.isnan(next_token_logits).any() or torch.isinf(next_token_logits).any():
                    next_token = self.eos_id  # æˆ– self.pad_idï¼Œä½†ç”¨ EOS æ›´åˆç†ï¼ˆæå‰ç»“æŸï¼‰
                else:
                    probs = F.softmax(next_token_logits, dim=-1)
                    if torch.isnan(probs).any() or probs.sum() <= 1e-8:
                        next_token = self.eos_id
                    else:
                        next_token = torch.multinomial(probs, num_samples=1).item()

                # ğŸ”’ å¼ºåˆ¶çº¦æŸ ID èŒƒå›´ [0, vocab_size)
                if next_token < 0 or next_token >= self.zh_sp.vocab_size():
                    next_token = self.eos_id  # å®‰å…¨ fallback

                tgt_indices.append(next_token)

                if next_token == self.eos_id:  # = 0
                    break

            # âœ… å…³é”®ä¿®å¤ï¼šåªå– SOS ä¹‹åã€EOS ä¹‹å‰çš„ token
            generated_ids = []
            for tid in tgt_indices[1:]:  # è·³è¿‡å¼€å¤´çš„ SOS (1)
                if tid == self.eos_id:   # é‡åˆ° EOS åœæ­¢ï¼Œä¸”ä¸åŒ…å«å®ƒ
                    break
                generated_ids.append(tid)

            # è§£ç ï¼šè·³è¿‡å¼€å¤´çš„ SOS (1)
            decoded = self.zh_sp.decode_ids(generated_ids)
            return decoded.strip()