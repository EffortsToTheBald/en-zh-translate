"""
ä½¿ç”¨ SentencePiece Tokenizer çš„ Transformer è®­ç»ƒè„šæœ¬
é€‚ç”¨äºä¸­è‹±æœºå™¨ç¿»è¯‘ä»»åŠ¡
"""

import os
import sys
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import time
import math
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼ˆä¾¿äºæ¨¡å—å¯¼å…¥ï¼‰
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config import Config
from src.dataset import create_dataloaders
from src.model import build_model
from src.utils import LabelSmoothingLoss, EarlyStopping

def get_transformer_lr(step: int, d_model: int, warmup_steps: int, max_lr: float = 0.0005):
    """åŸè®ºæ–‡å­¦ä¹ ç‡å…¬å¼"""
    if step == 0:
        return 1e-8
    lr = (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))
    return min(lr, max_lr)

def generate_square_subsequent_mask(sz):
    """ç”Ÿæˆ decoder æ‰€éœ€çš„ä¸‹ä¸‰è§’ maskï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥è¯"""
    mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)
    return mask

def train_epoch(model, train_loader, criterion, optimizer, device, epoch, pad_idx,global_step):
    model.train()
    total_loss = 0.0
    total_tokens = 0

    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch} Training")):
        # å…ˆæ›´æ–°å­¦ä¹ ç‡ï¼Œå†æ‰§è¡Œè®­ç»ƒæ­¥éª¤ï¼ˆä¿®å¤æ—¶æœºé—®é¢˜ï¼‰
        lr = get_transformer_lr(
            step=global_step,
            d_model=Config.D_MODEL,
            warmup_steps=Config.WARMUP_STEPS,
            max_lr=Config.MAX_LR
        )
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        
        if global_step % 480 == 0:
            theoretical_lr = get_transformer_lr(global_step, Config.D_MODEL, Config.WARMUP_STEPS)
            actual_lr = optimizer.param_groups[0]['lr']
            print(f"  [DEBUG] Step {global_step}: theoretical={theoretical_lr:.6f}, actual={actual_lr:.6f}")

        src = batch['src'].to(device)
        tgt = batch['tgt'].to(device)

        tgt_input = tgt[:, :-1]
        tgt_output = tgt[:, 1:]

        src_padding_mask = (src == pad_idx).to(device)
        tgt_padding_mask = (tgt_input == pad_idx).to(device)
        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)

        output, _ = model(
            src=src,
            tgt=tgt_input,
            tgt_mask=tgt_mask,
            src_padding_mask=src_padding_mask,
            tgt_padding_mask=tgt_padding_mask,
            memory_key_padding_mask=src_padding_mask  
        )

        output_flat = output.view(-1, output.size(-1))
        tgt_flat = tgt_output.reshape(-1)

        loss = criterion(output_flat, tgt_flat)
        if torch.isnan(loss) or torch.isinf(loss):
            print("âš ï¸ Loss is NaN or Inf! Skipping batch.")
            optimizer.zero_grad()
            continue
        ntokens = (tgt_flat != pad_idx).sum().item()

        optimizer.zero_grad()

        loss.backward()
        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), Config.CLIP_GRAD, norm_type=2.0)
        # æ¢¯åº¦çˆ†ç‚¸ä¿æŠ¤ï¼šå¦‚æœè£å‰ªåçš„æ¢¯åº¦èŒƒæ•°ä»ç„¶è¿‡å¤§ï¼Œè·³è¿‡æ›´æ–°
        if total_norm > 10:  # é˜ˆå€¼å¯æ ¹æ®æƒ…å†µè°ƒæ•´
            print(f"âš ï¸ æ¢¯åº¦çˆ†ç‚¸! Norm={total_norm:.2f}ï¼Œè·³è¿‡æœ¬è½®æ›´æ–°")
            optimizer.zero_grad()
            global_step += 1
            continue
        # elif total_norm > Config.CLIP_GRAD:
        #     print(f"âš ï¸ æ¢¯åº¦è£å‰ª! Norm={total_norm:.2f} (é˜ˆå€¼={Config.CLIP_GRAD})")
        optimizer.step()

        global_step += 1

        total_loss += loss.item() * ntokens
        total_tokens += ntokens

    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
    return avg_loss , global_step

def validate(model, val_loader, criterion, device, pad_idx):
    model.eval()
    total_loss = 0.0
    total_tokens = 0

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation", leave=False):
            src = batch['src'].to(device)
            tgt = batch['tgt'].to(device)

            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]

            src_padding_mask = (src == pad_idx).to(device)
            tgt_padding_mask = (tgt_input == pad_idx).to(device)
            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)

            output, _ = model(
                src=src,
                tgt=tgt_input,
                tgt_mask=tgt_mask,
                src_padding_mask=src_padding_mask,
                tgt_padding_mask=tgt_padding_mask,
                memory_key_padding_mask=src_padding_mask
            )

            output_flat = output.view(-1, output.size(-1))
            tgt_flat = tgt_output.reshape(-1)

            loss = criterion(output_flat, tgt_flat)
            ntokens = (tgt_flat != pad_idx).sum().item()

            total_loss += loss.item() * ntokens
            total_tokens += ntokens

    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
    return avg_loss

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ Transformer ä¸­è‹±ç¿»è¯‘è®­ç»ƒï¼ˆSentencePiece ç‰ˆï¼‰")
    Config.display()
    
    # åˆ›å»ºç›®å½•
    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(Config.LOG_DIR, exist_ok=True)
    
    # è®¾å¤‡
    device = Config.DEVICE
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # Step 1: åŠ è½½æ•°æ® + tokenizer
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†å’Œ Tokenizer...")
    train_loader, val_loader, en_tokenizer, zh_tokenizer = create_dataloaders(Config)
    print("ğŸ” Tokenizer è°ƒè¯•:")
    print(f"  EN PAD: '{en_tokenizer.id_to_piece(Config.PAD_IDX)}' (ID={Config.PAD_IDX})")
    print(f"  ZH PAD: '{zh_tokenizer.id_to_piece(Config.PAD_IDX)}' (ID={Config.PAD_IDX})")
    print(f"  EN SOS: '{en_tokenizer.id_to_piece(Config.SOS_IDX)}'")
    print(f"  ZH EOS: '{zh_tokenizer.id_to_piece(Config.EOS_IDX)}'")    
    assert zh_tokenizer.id_to_piece(Config.PAD_IDX) == "<pad>", "ä¸­æ–‡ PAD ID é”™è¯¯ï¼"
    Config.init_token_ids(en_tokenizer, zh_tokenizer)
    # Step 2: æ„å»ºæ¨¡å‹
    print("\nğŸ—ï¸  æ„å»ºæ¨¡å‹...")
    src_vocab_size = en_tokenizer.vocab_size
    tgt_vocab_size = zh_tokenizer.vocab_size
    print(f"è‹±æ–‡è¯æ±‡è¡¨: {src_vocab_size}")
    print(f"ä¸­æ–‡è¯æ±‡è¡¨: {tgt_vocab_size}")

    model = build_model(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        device=device
    )

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€» {total_params:,} | å¯è®­ç»ƒ {trainable_params:,}")
    
    # Step 3: æŸå¤±å‡½æ•° & ä¼˜åŒ–å™¨
    pad_idx = Config.PAD_IDX  
    criterion = LabelSmoothingLoss(
        tgt_vocab_size,
        padding_idx=pad_idx,
        smoothing=Config.LABEL_SMOOTHING
    )

    # def lr_lambda(current_step: int):
    #     """Transformer åŸç‰ˆ LR è°ƒåº¦ï¼ˆæ¯æ­¥è°ƒç”¨ï¼‰"""
    #     if current_step == 0:
    #         return 1e-8

    #     lr = (Config.D_MODEL ** -0.5) * min(
    #     current_step ** -0.5,
    #     current_step * (Config.WARMUP_STEPS ** -1.5)
    #     )
    #     return min(lr, 0.0004)

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=Config.INIT_LR,  
        betas=(0.9, 0.98),
        eps=1e-9,
        weight_decay=Config.WEIGHT_DECAY
    )

    # scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    # Step 5. TensorBoard````````````````````
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    writer = SummaryWriter(f"{Config.LOG_DIR}/{timestamp}")
    
    # Step 6. æ—©åœ
    early_stopping = EarlyStopping(
        patience=Config.PATIENCE,
        min_delta=Config.MIN_DELTA,
        verbose=True
    )
    
    # Step 7. è®­ç»ƒå¾ªç¯
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    best_val_loss = float('inf')
    global_step = 0
    print("âœ… Starting training with global_step = 0")   
    
    for epoch in range(1, Config.EPOCHS + 1):
        print(f"\nğŸ“… å¼€å§‹ç¬¬ {epoch}/{Config.EPOCHS} è½®è®­ç»ƒ")
        
        start_time = time.time()
        
        # âœ… è°ƒç”¨ train_epoch æ—¶ä¼ å…¥ schedulerï¼Œå¹¶æ¥æ”¶ lr
        train_loss, global_step  = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch, pad_idx,global_step 
        )        
                
        # éªŒè¯
        val_loss = validate(model, val_loader, criterion, device, pad_idx)        
        current_lr = optimizer.param_groups[0]['lr']
        epoch_time = time.time() - start_time
        
        # è®°å½•åˆ°TensorBoard
        writer.add_scalar('Loss/train', train_loss, epoch)
        writer.add_scalar('Loss/val', val_loss, epoch)
        writer.add_scalar('LR', current_lr, epoch)
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"  å­¦ä¹ ç‡: {current_lr:.8f}")
        print(f"  æ—¶é—´: {epoch_time:.1f}ç§’")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        is_best = val_loss < best_val_loss
        if is_best:
            best_val_loss = val_loss
            print(f"  ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±!")
        
        if is_best:
            # æ„å»ºå¯åºåˆ—åŒ–çš„é…ç½®
            config_save = {
                k: v for k, v in Config.__dict__.items()
                if not k.startswith('__') and isinstance(v, (int, float, str, bool))
            }
            # æ‰‹åŠ¨å¤„ç† DEVICE
            config_save['DEVICE'] = str(Config.DEVICE)
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                # 'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'best_val_loss': best_val_loss,
                'config': config_save
            }
            
            if is_best:
                torch.save(checkpoint, f"{Config.CHECKPOINT_DIR}/best_model.pth")
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
        
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss):
            print(f"\nâš ï¸  æ—©åœè§¦å‘ï¼Œåœ¨ epoch {epoch}")
            break
    
    writer.close()
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")

if __name__ == "__main__":
    main()