"""è®­ç»ƒæ¨¡å—"""
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import time
import math
from datetime import datetime

from config import Config
from vocabulary import Vocabulary
from dataset import create_dataloaders
from model import build_model
from utils import EarlyStopping, LabelSmoothingLoss

# def train_epoch(model, dataloader, criterion, optimizer, device, epoch):
#     """è®­ç»ƒä¸€ä¸ªepoch"""
#     model.train()
#     total_loss = 0
#     total_tokens = 0
    
#     progress_bar = tqdm(dataloader, desc=f"Epoch {epoch} [Train]", leave=False)
    
#     for batch_idx, batch in enumerate(progress_bar):
#         src = batch['src'].to(device)
#         tgt = batch['tgt'].to(device)
#         src_mask = batch['src_mask'].to(device)
#         tgt_mask = batch['tgt_mask'].to(device)
        
#         # å‡†å¤‡è¾“å…¥è¾“å‡º
#         tgt_input = tgt[:, :-1]
#         tgt_output = tgt[:, 1:]
        
#         # å‰å‘ä¼ æ’­
#         optimizer.zero_grad()
#         output, _ = model(src, tgt_input, src_mask, tgt_mask[:, :, :-1, :-1])
        
#         # è®¡ç®—æŸå¤±
#         loss = criterion(
#             output.contiguous().view(-1, output.size(-1)),
#             tgt_output.contiguous().view(-1)
#         )
        
#         # åå‘ä¼ æ’­
#         loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), Config.CLIP_GRAD)
#         optimizer.step()
        
#         # ç»Ÿè®¡
#         batch_tokens = (tgt_output != 0).sum().item()
#         total_loss += loss.item() * batch_tokens
#         total_tokens += batch_tokens
        
#         # æ›´æ–°è¿›åº¦æ¡
#         if batch_idx % 10 == 0:
#             progress_bar.set_postfix({
#                 'loss': loss.item(),
#                 'lr': optimizer.param_groups[0]['lr']
#             })
    
#     return total_loss / total_tokens if total_tokens > 0 else 0

# def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
#     model.train()
#     total_loss = 0
    
#     for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"è®­ç»ƒ epoch {epoch}")):
#         src = batch['src'].to(device)  # [batch_size, src_len]
#         tgt = batch['tgt'].to(device)  # [batch_size, tgt_len]
        
#         # åˆ›å»ºç›®æ ‡è¾“å…¥ï¼ˆå»æ‰æœ€åä¸€ä¸ªtokenï¼‰
#         tgt_input = tgt[:, :-1]  # [batch_size, tgt_len-1]
        
#         # åˆ›å»ºå¡«å……æ©ç ï¼ˆ2Då¼ é‡ï¼‰
#         src_padding_mask = (src == 0)  # [batch_size, src_len]
#         tgt_padding_mask = (tgt_input == 0)  # [batch_size, tgt_len-1]
        
#         # åˆ›å»ºå› æœæ©ç ï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
#         tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)  # [tgt_len-1, tgt_len-1]
        
#         # è°ƒç”¨æ¨¡å‹
#         output, _ = model(
#             src=src,
#             tgt=tgt_input,
#             tgt_mask=tgt_mask,
#             src_padding_mask=src_padding_mask,
#             tgt_padding_mask=tgt_padding_mask
#         )
        
#         # è®¡ç®—æŸå¤±
#         # output shape: [batch_size, tgt_len-1, vocab_size]
#         # target shape: [batch_size, tgt_len-1]
#         target = tgt[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªtokenï¼ˆ<sos>ï¼‰
        
#         output_flat = output.reshape(-1, output.size(-1))
#         target_flat = target.reshape(-1)
        
#         loss = criterion(output_flat, target_flat)
        
#         optimizer.zero_grad()
#         loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#         optimizer.step()
        
#         total_loss += loss.item()
        
#         # å¯é€‰ï¼šæ¯Nä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡æŸå¤±
#         if batch_idx % 100 == 0:
#             print(f"æ‰¹æ¬¡ {batch_idx}, æŸå¤±: {loss.item():.4f}")
    
#     return total_loss / len(train_loader)

def train_epoch(model, train_loader, criterion, optimizer, device, epoch, pad_idx):
    model.train()
    total_loss = 0
    total_tokens = 0  # æ–°å¢
    
    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"è®­ç»ƒ epoch {epoch}")):
        src = batch['src'].to(device)
        tgt = batch['tgt'].to(device)
        
        tgt_input = tgt[:, :-1]
        target = tgt[:, 1:]
        
        src_padding_mask = (src == 0)
        tgt_padding_mask = (tgt_input == 0)
        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)
        
        output, _ = model(
            src=src,
            tgt=tgt_input,
            tgt_mask=tgt_mask,
            src_padding_mask=src_padding_mask,
            tgt_padding_mask=tgt_padding_mask
        )
        
        # è®¡ç®—æŸå¤±ï¼ˆLabelSmoothingLoss å·²å¿½ç•¥ pad_idxï¼‰
        output_flat = output.reshape(-1, output.size(-1))
        target_flat = target.reshape(-1)
        loss = criterion(output_flat, target_flat)
        
        # ç»Ÿè®¡é PAD token æ•°é‡
        ntokens = (target_flat != pad_idx).sum().item()  # æ³¨æ„ï¼šéœ€ä¼ å…¥ pad_idx æˆ–ä» criterion è·å–
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Config.CLIP_GRAD)  # ä½¿ç”¨é…ç½®
        optimizer.step()
        
        total_loss += loss.item() * ntokens
        total_tokens += ntokens
        
        if batch_idx % 100 == 0:
            print(f"æ‰¹æ¬¡ {batch_idx}, æŸå¤±: {loss.item():.4f}")
    
    return total_loss / total_tokens if total_tokens > 0 else 0

def generate_square_subsequent_mask(sz):
    """ç”Ÿæˆå› æœæ©ç ï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰"""
    return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)

# def validate(model, dataloader, criterion, device):
#     """éªŒè¯"""
#     model.eval()
#     total_loss = 0
#     total_tokens = 0
    
#     with torch.no_grad():
#         for batch in tqdm(dataloader, desc="éªŒè¯", leave=False):
#             src = batch['src'].to(device)
#             tgt = batch['tgt'].to(device)
#             src_mask = batch['src_mask'].to(device)
#             tgt_mask = batch['tgt_mask'].to(device)
            
#             tgt_input = tgt[:, :-1]
#             tgt_output = tgt[:, 1:]
            
#             output, _ = model(src, tgt_input, src_mask, tgt_mask[:, :, :-1, :-1])
            
#             loss = criterion(
#                 output.contiguous().view(-1, output.size(-1)),
#                 tgt_output.contiguous().view(-1)
#             )
            
#             batch_tokens = (tgt_output != 0).sum().item()
#             total_loss += loss.item() * batch_tokens
#             total_tokens += batch_tokens
    
#     return total_loss / total_tokens if total_tokens > 0 else 0

def validate(model, dataloader, criterion, device):
    """éªŒè¯ - ä¸ train_epoch ä½¿ç”¨ç›¸åŒçš„ mask æ„é€ æ–¹å¼"""
    model.eval()
    total_loss = 0
    total_batches = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="éªŒè¯", leave=False):
            src = batch['src'].to(device)  # [B, S]
            tgt = batch['tgt'].to(device)  # [B, T]
            
            tgt_input = tgt[:, :-1]        # [B, T-1]
            target = tgt[:, 1:]            # [B, T-1]
            
            # æ„é€  masks
            src_padding_mask = (src == 0)                     # [B, S]
            tgt_padding_mask = (tgt_input == 0)               # [B, T-1]
            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)  # [T-1, T-1]
            
            # å‰å‘ä¼ æ’­
            output, _ = model(
                src=src,
                tgt=tgt_input,
                tgt_mask=tgt_mask,
                src_padding_mask=src_padding_mask,
                tgt_padding_mask=tgt_padding_mask
            )
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output.reshape(-1, output.size(-1)), target.reshape(-1))
            total_loss += loss.item()
            total_batches += 1
    
    return total_loss / total_batches if total_batches > 0 else 0

def translate_example(model, sentence, en_vocab, zh_vocab, device, temperature=0.8):
    """ç¿»è¯‘ç¤ºä¾‹"""
    model.eval()
    
    # ç¼–ç è¾“å…¥
    src_indices = en_vocab.encode(sentence, add_special_tokens=True)
    src = torch.tensor(src_indices).unsqueeze(0).to(device)
    src_mask = torch.ones(1, 1, 1, len(src_indices)).bool().to(device)
    
    # ç¼–ç å™¨è¾“å‡º
    with torch.no_grad():
        encoder_output = model.encode(src, src_mask)
        
        # åˆå§‹åŒ–ç›®æ ‡åºåˆ—
        tgt_indices = [zh_vocab.word2idx[Config.SOS_TOKEN]]
        
        for i in range(Config.MAX_LENGTH):
            tgt = torch.tensor(tgt_indices).unsqueeze(0).to(device)
            
            # åˆ›å»ºå› æœæ©ç 
            tgt_len = len(tgt_indices)
            tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).unsqueeze(0).unsqueeze(0).bool().to(device)
            tgt_mask = tgt_mask & (tgt != zh_vocab.word2idx[Config.PAD_TOKEN]).unsqueeze(1).unsqueeze(2)
            
            # è§£ç 
            decoder_output, _ = model.decode(tgt, encoder_output, src_mask, tgt_mask)
            output = model.output_layer(decoder_output)
            
            # åº”ç”¨æ¸©åº¦é‡‡æ ·
            output = output / temperature
            probs = F.softmax(output[0, -1], dim=-1)
            next_token = torch.multinomial(probs, 1).item()
            
            tgt_indices.append(next_token)
            
            # é‡åˆ°EOSåˆ™åœæ­¢
            if next_token == zh_vocab.word2idx[Config.EOS_TOKEN]:
                break
        
        # è§£ç ä¸ºæ–‡æœ¬
        translation = zh_vocab.decode(tgt_indices[1:-1])
        
        return translation

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ Transformerç¿»è¯‘æ¨¡å‹è®­ç»ƒ")
    Config.display()
    
    # åˆ›å»ºç›®å½•
    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(Config.LOG_DIR, exist_ok=True)
    
    # è®¾å¤‡
    device = Config.DEVICE
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½è¯æ±‡è¡¨
    print("\nğŸ”¤ åŠ è½½è¯æ±‡è¡¨...")
    en_vocab = Vocabulary.load(f"{Config.VOCAB_DIR}/en_vocab.pkl")
    zh_vocab = Vocabulary.load(f"{Config.VOCAB_DIR}/zh_vocab.pkl")
    
    print(f"è‹±æ–‡è¯æ±‡è¡¨: {len(en_vocab)}")
    print(f"ä¸­æ–‡è¯æ±‡è¡¨: {len(zh_vocab)}")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_dataloaders(en_vocab, zh_vocab, Config)

    for batch in train_loader:
        print(f"src shape: {batch['src'].shape}")  # åº”è¯¥æ˜¯ [batch_size, seq_len]
        print(f"tgt shape: {batch['tgt'].shape}")
        break    

    # 3. æ„å»ºæ¨¡å‹
    print("\nğŸ—ï¸  æ„å»ºæ¨¡å‹...")
    model = build_model(len(en_vocab), len(zh_vocab), device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ€»å‚æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # 4. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    # pad_idx = en_vocab.word2idx[Config.PAD_TOKEN]
    pad_idx = zh_vocab.word2idx[Config.PAD_TOKEN]
    criterion = LabelSmoothingLoss(
        len(zh_vocab),
        padding_idx=pad_idx,
        smoothing=Config.LABEL_SMOOTHING
    )
    
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=Config.INIT_LR,
        betas=(0.9, 0.98),
        eps=1e-9,
        weight_decay=Config.WEIGHT_DECAY
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if Config.LR_SCHEDULER == "cosine":
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=Config.T_MAX, T_mult=2
        )
    else:
        def lr_lambda(step):
            if step < Config.WARMUP_STEPS:
                return float(step) / float(max(1, Config.WARMUP_STEPS))
            else:
                progress = (step - Config.WARMUP_STEPS) / (Config.EPOCHS * len(train_loader) - Config.WARMUP_STEPS)
                return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))
        
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # 5. TensorBoard
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    writer = SummaryWriter(f"{Config.LOG_DIR}/{timestamp}")
    
    # 6. æ—©åœ
    early_stopping = EarlyStopping(
        patience=Config.PATIENCE,
        min_delta=Config.MIN_DELTA
    )
    
    # 7. è®­ç»ƒå¾ªç¯
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    best_val_loss = float('inf')
    
    for epoch in range(1, Config.EPOCHS + 1):
        print(f"\n{'='*50}")
        print(f"Epoch {epoch}/{Config.EPOCHS}")
        print(f"{'='*50}")
        
        start_time = time.time()
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch, pad_idx)
        
        # éªŒè¯
        val_loss = validate(model, val_loader, criterion, device)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        epoch_time = time.time() - start_time
        
        # è®°å½•åˆ°TensorBoard
        writer.add_scalar('Loss/train', train_loss, epoch)
        writer.add_scalar('Loss/val', val_loss, epoch)
        writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        print(f"  æ—¶é—´: {epoch_time:.1f}ç§’")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        is_best = val_loss < best_val_loss
        if is_best:
            best_val_loss = val_loss
            print(f"  ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±!")
        
        if epoch % 5 == 0 or is_best:
            # æ„å»ºå¯åºåˆ—åŒ–çš„é…ç½®
            config_save = {
                k: v for k, v in Config.__dict__.items()
                if not k.startswith('__') and isinstance(v, (int, float, str, bool))
            }
            # æ‰‹åŠ¨å¤„ç† DEVICE
            config_save['DEVICE'] = str(Config.DEVICE)
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'best_val_loss': best_val_loss,
                # 'en_vocab': en_vocab,
                # 'zh_vocab': zh_vocab,
                'config': config_save
            }
            
            if is_best:
                torch.save(checkpoint, f"{Config.CHECKPOINT_DIR}/best_model.pth")
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
            
            if epoch % 20 == 0:
                torch.save(checkpoint, f"{Config.CHECKPOINT_DIR}/checkpoint_epoch_{epoch}.pth")
                print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹")
        
        # æ¯5è½®æ˜¾ç¤ºç¿»è¯‘ç¤ºä¾‹
        # if epoch % 5 == 0:
        #     print(f"\nğŸ” ç¿»è¯‘ç¤ºä¾‹:")
        #     test_sentences = [
        #         "A dog is running in the park.",
        #         "Two people are building a snow house.",
        #         "A woman is cooking in the kitchen."
        #     ]
            
        #     for sentence in test_sentences:
        #         try:
        #             translation = translate_example(model, sentence, en_vocab, zh_vocab, device)
        #             print(f"  '{sentence}'")
        #             print(f"  -> '{translation}'")
        #         except Exception as e:
        #             print(f"  ç¿»è¯‘å¤±è´¥: {e}")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss):
            print(f"\nâš ï¸  æ—©åœè§¦å‘ï¼Œåœ¨ epoch {epoch}")
            break
    
    writer.close()
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")

if __name__ == "__main__":
    main()