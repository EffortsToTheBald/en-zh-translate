"""æ•°æ®é›†æ¨¡å—"""
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

class ParallelDataset(Dataset):
    """å¹³è¡Œæ–‡æœ¬æ•°æ®é›†"""
    
    def __init__(self, en_file, zh_file, en_vocab, zh_vocab, max_length=50):
        self.data = []
        self.en_vocab = en_vocab
        self.zh_vocab = zh_vocab
        self.max_length = max_length
        
        print(f"ğŸ“– åŠ è½½æ•°æ®é›†: {en_file}")
        
        # ç»Ÿè®¡è¡Œæ•°
        with open(en_file, 'r', encoding='utf-8') as f_en, \
             open(zh_file, 'r', encoding='utf-8') as f_zh:
            
            en_lines = sum(1 for _ in f_en)
            zh_lines = sum(1 for _ in f_zh)
        
        lines = min(en_lines, zh_lines)
        print(f"  æ€»è¡Œæ•°: {lines}")
        
        # è¯»å–å’Œç¼–ç 
        skipped = 0
        with open(en_file, 'r', encoding='utf-8') as f_en, \
             open(zh_file, 'r', encoding='utf-8') as f_zh:
            
            for i, (en_line, zh_line) in enumerate(tqdm(zip(f_en, f_zh), total=lines, desc="ç¼–ç ")):
                en_text = en_line.strip()
                zh_text = zh_line.strip()
                
                if not en_text or not zh_text:
                    skipped += 1
                    continue
                
                # ç¼–ç 
                en_indices = en_vocab.encode(en_text, add_special_tokens=True)
                zh_indices = zh_vocab.encode(zh_text, add_special_tokens=True)
                
                # æ£€æŸ¥é•¿åº¦
                if (len(en_indices) <= max_length and 
                    len(zh_indices) <= max_length):
                    
                    # æ£€æŸ¥æœªçŸ¥è¯æ¯”ä¾‹
                    unk_idx = en_vocab.word2idx[en_vocab.UNK_TOKEN]
                    en_unk_ratio = en_indices.count(unk_idx) / len(en_indices)
                    zh_unk_ratio = zh_indices.count(unk_idx) / len(zh_indices)
                    
                    if en_unk_ratio < 0.3 and zh_unk_ratio < 0.3:
                        self.data.append({
                            'src': torch.tensor(en_indices),
                            'tgt': torch.tensor(zh_indices),
                            'src_text': en_text,
                            'tgt_text': zh_text,
                            'src_len': len(en_indices),
                            'tgt_len': len(zh_indices)
                        })
                    else:
                        skipped += 1
                else:
                    skipped += 1
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  æœ‰æ•ˆæ ·æœ¬: {len(self.data)}")
        print(f"  è·³è¿‡æ ·æœ¬: {skipped}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•° - ä»…è¿”å›å¡«å……åçš„åºåˆ—ï¼Œmask åœ¨è®­ç»ƒæ—¶åŠ¨æ€ç”Ÿæˆ"""
    srcs = [item['src'] for item in batch]
    tgts = [item['tgt'] for item in batch]
    
    src = torch.nn.utils.rnn.pad_sequence(srcs, batch_first=True, padding_value=0)
    tgt = torch.nn.utils.rnn.pad_sequence(tgts, batch_first=True, padding_value=0)
    
    return {
        'src': src,
        'tgt': tgt
    }
    # """æ‰¹å¤„ç†å‡½æ•°"""
    # srcs = [item['src'] for item in batch]
    # tgts = [item['tgt'] for item in batch]
    
    # src = torch.nn.utils.rnn.pad_sequence(srcs, batch_first=True, padding_value=0)
    # tgt = torch.nn.utils.rnn.pad_sequence(tgts, batch_first=True, padding_value=0)
    
    # src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
    # tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)
    
    # seq_len = tgt.size(1)
    # nopeak_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()
    # tgt_mask = tgt_mask & nopeak_mask
    
    # return {
    #     'src': src,
    #     'tgt': tgt,
    #     'src_mask': src_mask,
    #     'tgt_mask': tgt_mask
    # }

def create_dataloaders(en_vocab, zh_vocab, config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    from config import Config
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = ParallelDataset(
        Config.TRAIN_EN_FILE,
        Config.TRAIN_ZH_FILE,
        en_vocab, zh_vocab,
        max_length=Config.MAX_LENGTH
    )
    
    val_dataset = ParallelDataset(
        Config.VAL_EN_FILE,
        Config.VAL_ZH_FILE,
        en_vocab, zh_vocab,
        max_length=Config.MAX_LENGTH
    )
    
    if len(train_dataset) == 0 or len(val_dataset) == 0:
        raise ValueError("æ•°æ®é›†ä¸ºç©ºï¼")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=Config.NUM_WORKERS,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=Config.NUM_WORKERS,
        pin_memory=True
    )
    
    print(f"ğŸ“Š æ•°æ®åŠ è½½å™¨ç»Ÿè®¡:")
    print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)} (æ‰¹æ¬¡å¤§å°: {Config.BATCH_SIZE})")
    print(f"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)} (æ‰¹æ¬¡å¤§å°: {Config.BATCH_SIZE})")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    
    return train_loader, val_loader