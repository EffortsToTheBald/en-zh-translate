"""
ä¿®å¤ç‰ˆè¯Šæ–­è„šæœ¬ï¼šè§£å†³é‡å¤ç”Ÿæˆ + ä¼˜åŒ–è§£ç é€»è¾‘
"""
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import torch
import sentencepiece as spm
from src.config import Config

from src.model import build_model


def generate_square_subsequent_mask(sz, device):
    mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1)
    return mask.bool()


def load_model_and_tokenizers(device, checkpoint_path):
    """ç²¾ç®€ç‰ˆåŠ è½½å‡½æ•°"""
    print(f"ğŸ“‚ åŠ è½½ checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    saved_config = checkpoint['config']
    
    vocab_dir = saved_config['VOCAB_DIR']
    en_sp = spm.SentencePieceProcessor()
    zh_sp = spm.SentencePieceProcessor()
    en_sp.load(os.path.join(vocab_dir, "en.model"))
    zh_sp.load(os.path.join(vocab_dir, "ch.model"))

    model = build_model(
        src_vocab_size=en_sp.vocab_size(),
        tgt_vocab_size=zh_sp.vocab_size(),
        device=device,** saved_config
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    sos_idx = saved_config['SOS_IDX']
    eos_idx = saved_config['EOS_IDX']
    pad_idx = saved_config['PAD_IDX']

    return model, en_sp, zh_sp, sos_idx, eos_idx, pad_idx


def translate_sentence_debug(model, sentence, en_sp, zh_sp, sos_idx, eos_idx, pad_idx, device, max_len=50, repetition_penalty=1.2):
    """å¸¦é‡å¤æƒ©ç½šçš„ä¼˜åŒ–è§£ç å‡½æ•°"""
    print(f"\n--- ç¿»è¯‘è¾“å…¥: '{sentence}' ---")
    
    # 1. ç¼–ç æºå¥
    src_tokens = en_sp.encode(sentence, out_type=int)
    print(f"  [EN] Tokens: {src_tokens}")
    print(f"  [EN] Decoded back: '{en_sp.decode_ids(src_tokens)}'")
    
    if not src_tokens:
        print("  âš ï¸ æºå¥ç¼–ç ä¸ºç©ºï¼")
        return "", []

    src = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0).to(device)

    # 2. è§£ç è¿‡ç¨‹ï¼ˆå¸¦é‡å¤æƒ©ç½šï¼‰
    tgt_indices = [sos_idx]
    print(f"  [ZH] åˆå§‹ tgt: {tgt_indices} (SOS={sos_idx})")
    
    # è®°å½•å·²ç”Ÿæˆçš„tokenï¼Œç”¨äºé‡å¤æƒ©ç½š
    generated_tokens = set()
    consecutive_repeats = 0
    last_token = None

    with torch.no_grad():
        for step in range(max_len - 1):
            tgt = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device)
            src_padding_mask = (src == pad_idx)
            tgt_padding_mask = (tgt == pad_idx)
            tgt_mask = generate_square_subsequent_mask(tgt.size(1), device)

            output, _ = model(
                src=src,
                tgt=tgt,
                tgt_mask=tgt_mask,
                src_padding_mask=src_padding_mask,
                tgt_padding_mask=tgt_padding_mask,
                memory_key_padding_mask=src_padding_mask
            )
            
            # è·å–æœ€åä¸€ä¸ªtokençš„logits
            logits = output[0, -1]
            
            # é‡å¤æƒ©ç½šï¼šé™ä½å·²ç”Ÿæˆtokençš„æ¦‚ç‡
            for token in generated_tokens:
                if logits[token] > 0:
                    logits[token] /= repetition_penalty
                else:
                    logits[token] *= repetition_penalty
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            next_token = logits.argmax().item()
            
            # æ£€æŸ¥è¿ç»­é‡å¤
            if next_token == last_token:
                consecutive_repeats += 1
                if consecutive_repeats >= 3:  # è¿ç»­3æ¬¡ç”Ÿæˆç›¸åŒtokenåˆ™ç»ˆæ­¢
                    print(f"  âš ï¸ è¿ç»­é‡å¤ç”Ÿæˆtoken {next_token}ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                    break
            else:
                consecutive_repeats = 0
            
            last_token = next_token
            generated_tokens.add(next_token)
            tgt_indices.append(next_token)

            # æ‰“å°å‰5æ­¥
            if step < 5:
                piece = zh_sp.id_to_piece(next_token)
                print(f"    Step {step+1}: predicted ID={next_token}, piece='{piece}'")

            # ç»ˆæ­¢æ¡ä»¶
            if next_token == eos_idx or len(tgt_indices) >= max_len:
                break

    print(f"  [ZH] æœ€ç»ˆ IDs: {tgt_indices}")
    
    # 3. å®‰å…¨è§£ç 
    clean_ids = []
    for tid in tgt_indices:
        if tid in {sos_idx, eos_idx, pad_idx, zh_sp.unk_id()}:
            continue
        # è¿‡æ»¤è¿ç»­é‡å¤çš„token
        if clean_ids and clean_ids[-1] == tid:
            continue
        clean_ids.append(tid)
    
    print(f"  [ZH] Clean IDs: {clean_ids}")
    
    try:
        decoded = zh_sp.decode_ids(clean_ids).strip()
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        decoded = decoded.replace('â–', ' ').replace('  ', ' ').strip()
        print(f"  [ZH] æœ€ç»ˆè¯‘æ–‡: '{decoded}'")
        return decoded, tgt_indices
    except Exception as e:
        print(f"  âŒ è§£ç å¤±è´¥: {e}")
        return "", tgt_indices


def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

    checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, "best_model.pth")
    assert os.path.exists(checkpoint_path), f"âŒ Checkpoint ä¸å­˜åœ¨: {checkpoint_path}"

    # åŠ è½½
    model, en_sp, zh_sp, sos_idx, eos_idx, pad_idx = load_model_and_tokenizers(device, checkpoint_path)
    print("âœ… æ¨¡å‹ transformer.batch_first =", model.transformer.batch_first)
    
    # å…³é”®è¯Šæ–­
    print("\nğŸ” Special Token æ£€æŸ¥:")
    try:
        print(f"  SOS (ID={sos_idx}): '{en_sp.id_to_piece(sos_idx)}' (EN) | '{zh_sp.id_to_piece(sos_idx)}' (ZH)")
        print(f"  EOS (ID={eos_idx}): '{en_sp.id_to_piece(eos_idx)}' (EN) | '{zh_sp.id_to_piece(eos_idx)}' (ZH)")
        print(f"  PAD (ID={pad_idx}): '{en_sp.id_to_piece(pad_idx)}' (EN) | '{zh_sp.id_to_piece(pad_idx)}' (ZH)")
        print(f"  UNK (ID={en_sp.unk_id()}): '{en_sp.id_to_piece(en_sp.unk_id())}' (EN)")
        print(f"  UNK (ID={zh_sp.unk_id()}): '{zh_sp.id_to_piece(zh_sp.unk_id())}' (ZH)")
    except Exception as e:
        print(f"  âŒ è·å– special token å¤±è´¥: {e}")

    # æµ‹è¯•ç¿»è¯‘
    test_sentences = [
        "Hello world.",
        "I love you.",
        "What is your name?",
        "The weather is nice today.",
        "Good morning!"
    ]

    print("\nğŸ§ª å¼€å§‹æµ‹è¯•ç¿»è¯‘...")
    results = []
    for sent in test_sentences:
        pred, ids = translate_sentence_debug(
            model, sent, en_sp, zh_sp, sos_idx, eos_idx, pad_idx, 
            device, max_len=30, repetition_penalty=1.5  # å¢åŠ é‡å¤æƒ©ç½š
        )
        results.append((sent, pred))
        if not pred or "<unk>" in pred:
            print("  âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°æ— æ•ˆè¾“å‡ºï¼")

    # æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ“Š ç¿»è¯‘ç»“æœæ±‡æ€»:")
    for en, zh in results:
        print(f"  EN: {en}")
        print(f"  ZH: {zh}\n")

    all_bad = all(not zh or "<unk>" in zh or len(zh) > 50 for _, zh in results)
    if all_bad:
        print("âŒ è¯Šæ–­ç»“è®º: æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼å»ºè®®ï¼š")
        print("   1. å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆè‡³å°‘20è½®ï¼‰")
        print("   2. é™ä½å­¦ä¹ ç‡æˆ–è°ƒæ•´æ‰¹æ¬¡å¤§å°")
        print("   3. æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡")
    else:
        print("âœ… è¯Šæ–­ç»“è®º: æ¨¡å‹èƒ½è¾“å‡ºæœ‰æ•ˆä¸­æ–‡ï¼Œç¿»è¯‘è´¨é‡å¯é€šè¿‡æ›´å¤šè®­ç»ƒä¼˜åŒ–ã€‚")


if __name__ == "__main__":
    main()