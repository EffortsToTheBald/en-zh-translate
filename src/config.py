"""é…ç½®æ–‡ä»¶"""
import torch

class Config:
    """è®­ç»ƒé…ç½®"""
    
    # æ•°æ®è·¯å¾„
    TRAIN_EN_FILE = "data/train.en"
    TRAIN_ZH_FILE = "data/train.zh"
    VAL_EN_FILE = "data/val.en"
    VAL_ZH_FILE = "data/val.zh"
    
    # è¯æ±‡è¡¨è·¯å¾„
    VOCAB_DIR = "vocab"
    
    # æ¨¡å‹å‚æ•°
    D_MODEL = 512
    N_HEAD = 8
    NUM_ENCODER_LAYERS = 4
    NUM_DECODER_LAYERS = 4
    DIM_FEEDFORWARD = 1536
    DROPOUT = 0.15
    
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE = 64
    EPOCHS = 80
    INIT_LR = 0.00001
    MAX_LR = 0.0003
    WARMUP_STEPS = 3000 # æ€»æ­¥æ•° â‰ˆ 100 * 480 = 48,000 â†’ 5% æ˜¯ 2400ï¼Œä½†å°æ•°æ®é›†å¯æ›´æ¿€è¿›ï¼ˆ1000 è¶³å¤Ÿï¼‰
    WEIGHT_DECAY = 0.0005
    CLIP_GRAD = 1
    
    # å­¦ä¹ ç‡è°ƒåº¦
    LR_SCHEDULER = "transformer"
    T_MAX = 100
    
    # æ•°æ®å‚æ•°
    MAX_LENGTH = 80
    MAX_VOCAB_ZH = 16000
    MAX_VOCAB = 28000
    
    # æ ‡ç­¾å¹³æ»‘
    LABEL_SMOOTHING = 0.15
    
    # ç‰¹æ®Šæ ‡è®°
    PAD_TOKEN = "<pad>"
    SOS_TOKEN = "<s>"
    EOS_TOKEN = "</s>"
    UNK_TOKEN = "<unk>"

    PAD_IDX = 3
    UNK_IDX = 2
    SOS_IDX = 1
    EOS_IDX = 0

    # ç›®å½•
    CHECKPOINT_DIR = "checkpoints_new"
    LOG_DIR = "logs_new"
    RESULTS_DIR = "results_new"
    
    # è®¾å¤‡
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    NUM_WORKERS = 4
    
    # æ—©åœ
    PATIENCE = 20
    MIN_DELTA = 0.0001
    
    @classmethod
    def display(cls):
        """æ˜¾ç¤ºé…ç½®"""
        print("ğŸ“‹ è®­ç»ƒé…ç½®:")
        print("=" * 60)
        print(f"  è®­ç»ƒæ•°æ®: {cls.TRAIN_EN_FILE}")
        print(f"  éªŒè¯æ•°æ®: {cls.VAL_EN_FILE}")
        print(f"  æ¨¡å‹å¤§å°: d_model={cls.D_MODEL}, layers={cls.NUM_ENCODER_LAYERS}")
        print(f"  æ‰¹æ¬¡å¤§å°: {cls.BATCH_SIZE}")
        print(f"  è®­ç»ƒè½®æ•°: {cls.EPOCHS}")
        print(f"  å­¦ä¹ ç‡: {cls.INIT_LR}")
        print(f"  è®¾å¤‡: {cls.DEVICE}")
        print("=" * 60)

    @classmethod
    def init_token_ids(cls, en_tokenizer, zh_tokenizer):
        """ä» tokenizer åŠ¨æ€è·å–ç‰¹æ®Š token ID"""
        cls.PAD_IDX = en_tokenizer.piece_to_id("<pad>")
        cls.UNK_IDX = en_tokenizer.piece_to_id("<unk>")
        cls.SOS_IDX = en_tokenizer.piece_to_id("<s>")
        cls.EOS_IDX = en_tokenizer.piece_to_id("</s>")
        
        # éªŒè¯ä¸­è‹±æ–‡ tokenizer ä¸€è‡´æ€§
        assert cls.PAD_IDX == zh_tokenizer.piece_to_id("<pad>")
        assert cls.SOS_IDX == zh_tokenizer.piece_to_id("<s>")
        assert cls.EOS_IDX == zh_tokenizer.piece_to_id("</s>")
        print(f"âœ… Token IDs: PAD={cls.PAD_IDX}, SOS={cls.SOS_IDX}, EOS={cls.EOS_IDX}, UNK={cls.UNK_IDX}")