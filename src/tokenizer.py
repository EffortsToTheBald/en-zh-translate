"""
SentencePiece Tokenizer å°è£…
æ”¯æŒ BPE åˆ†è¯ï¼Œè‡ªåŠ¨å¤„ç† <s>, </s>, <pad>, <unk>
"""

import sentencepiece as spm
import os
from typing import List, Union

class SentencePieceTokenizer:
    def __init__(self, model_path: str):
        self.sp = spm.SentencePieceProcessor()
        self.sp.load(model_path)
    
    def encode(self, text: str, out_type=str) -> Union[List[str], List[int]]:
        """ç¼–ç æ–‡æœ¬ä¸º tokens æˆ– IDs"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        return self.sp.encode(text, out_type=out_type)
    
    def decode(self, tokens: Union[List[str], List[int]]) -> str:
        """è§£ç  tokens æˆ– IDs ä¸ºæ–‡æœ¬"""
        if not tokens:
            return ""
        if isinstance(tokens[0], int):
            return self.sp.decode_ids(tokens)
        else:
            return self.sp.decode_pieces(tokens)

    # ğŸ‘‡ å¿…é¡»æ·»åŠ è¿™ä¸¤ä¸ªæ–¹æ³•ç”¨äºè°ƒè¯•å’ŒåŠ¨æ€è·å–ç‰¹æ®Š token ID
    def id_to_piece(self, idx):
        return self.sp.id_to_piece(idx)

    def piece_to_id(self, piece):
        return self.sp.piece_to_id(piece)

    @property
    def vocab_size(self) -> int:
        return self.sp.vocab_size()
    
    @property
    def pad_id(self) -> int:
        return self.sp.pad_id()
    
    @property
    def unk_id(self) -> int:
        return self.sp.unk_id()
    
    @property
    def bos_id(self) -> int:
        return self.sp.bos_id()
    
    @property
    def eos_id(self) -> int:
        return self.sp.eos_id()

def train_sentencepiece_tokenizers(
    en_corpus: str,
    zh_corpus: str,
    output_dir: str,
    vocab_size_en: int = 32000,
    vocab_size_zh: int = 16000
):
    """
    è®­ç»ƒè‹±æ–‡å’Œä¸­æ–‡çš„ SentencePiece æ¨¡å‹
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # è®­ç»ƒè‹±æ–‡ (BPE)
    spm.SentencePieceTrainer.train(
        input=en_corpus,
        model_prefix=os.path.join(output_dir, "en"),
        vocab_size=vocab_size_en,
        model_type="bpe",
        character_coverage=1.0,
        pad_id=3, unk_id=2, bos_id=1, eos_id=0,
        user_defined_symbols=["<pad>", "<s>", "</s>"]
    )
    
    # è®­ç»ƒä¸­æ–‡ (BPE)
    spm.SentencePieceTrainer.train(
        input=zh_corpus,
        model_prefix=os.path.join(output_dir, "ch"),
        vocab_size=vocab_size_zh,
        model_type="bpe",
        character_coverage=1.0,
        pad_id=3, unk_id=2, bos_id=1, eos_id=0,
        user_defined_symbols=["<pad>", "<s>", "</s>"]
    )
    
    print(f"âœ… è‹±æ–‡ tokenizer ä¿å­˜è‡³: {output_dir}/en.model")
    print(f"âœ… ä¸­æ–‡ tokenizer ä¿å­˜è‡³: {output_dir}/ch.model")